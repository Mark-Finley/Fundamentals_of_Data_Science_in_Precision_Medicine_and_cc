{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b8d7d2",
   "metadata": {
    "id": "05b8d7d2"
   },
   "source": [
    "<img src=\"materials/images/introduction-to-medical-imaging.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6365cc",
   "metadata": {
    "id": "1b6365cc"
   },
   "source": [
    "# Introduction to Medical Imaging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cba3d3b",
   "metadata": {
    "id": "7cba3d3b"
   },
   "source": [
    "Medical imaging is the field of medicine related to generating representative images of the internal structures of the body. The field is traditionally referred to as radiology. This is because for years the only imaging technique available was radiography (i.e. the use of radiation to produce images).\n",
    "\n",
    "Over the past several decades, additional techniques have been developed that don't use any form of radiation at all.  As a result, the term \"medical imaging\" is being increasingly adopted. Medical imaging is an important part of diagnostic and interventional medicine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bcc78d",
   "metadata": {
    "id": "32bcc78d"
   },
   "source": [
    "# Common medical imaging techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276f05df",
   "metadata": {
    "id": "276f05df"
   },
   "source": [
    "## Radiography"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dbe739",
   "metadata": {
    "id": "c5dbe739"
   },
   "source": [
    "<img src=\"materials/images/radiography.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3131b0f4",
   "metadata": {
    "id": "3131b0f4"
   },
   "source": [
    "During a radiographic procedure (X-ray), an X-ray beam is passed through the body to a detector so that an image is produced. It is used to visualize the internal parts of the body to assess disease, structural damage, or anomaly.\n",
    "\n",
    "For any given image, X-ray beams are passing through the entire specimen against a screen. Therefore, the image produced is a representation of a three-dimensional object compressed into two-dimensional space.\n",
    "\n",
    "The images show the parts of your body in different shades of black and white. This is because different tissues absorb different amounts of radiation. Bones absorb the most X-rays, so they look white. Fat and other soft tissues absorb less and look gray. Air absorbs the least, so lungs look black.\n",
    "\n",
    "There are 4 basic radiographic (X-ray) densities or shades of gray:\n",
    "* Air (black)\n",
    "* Fat (dark gray)\n",
    "* Water and most soft tissues (gray)\n",
    "* Bone and Metal (light gray to white)\n",
    "\n",
    "A limitation to radiographic imaging  is that soft tissue is produced in low resolution. However, bone tissue is ideally assessed in radiographs (e.g., fractures, arthritis, osteoporosis)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48d4f49",
   "metadata": {
    "id": "f48d4f49"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa5e11e",
   "metadata": {
    "id": "5fa5e11e"
   },
   "source": [
    "## Computed Axial Tomography (CAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06d6fe7",
   "metadata": {
    "id": "f06d6fe7"
   },
   "source": [
    "<img src=\"materials/images/cat.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a595dccb",
   "metadata": {
    "id": "a595dccb"
   },
   "source": [
    "Computed Axial Tomography (CAT) is a modification of radiography where the patient is positioned within the center of a ring and an X-ray beam is rotated around the patient. Each full revolution scans a narrow, horizontal, cross-sectional image (\"slice\") of the body. The control system moves the platform farther into the hole in order to scan the next slice. The slices in the image above, viewed from the bottom up (as if the viewer is standing at the patient's feet, looking towards their head), are each akin to a slice of bread. The combined images would form the \"loaf\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317d205f",
   "metadata": {
    "id": "317d205f"
   },
   "source": [
    "<img src=\"materials/images/cat-demonstration.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09065d21",
   "metadata": {
    "id": "09065d21"
   },
   "source": [
    "A series of radiographs are taken at different positions, providing radiographic images at multiple angles. A CAT (or CT) scan enables doctors to examine organs, bones, and other tissues one narrow slice at a time for abnormalities. Further, radiologists can use computers to combine the slices to create very detailed three-dimensional models of a patient's insides.\n",
    "\n",
    "Since they examine the body slice by slice, from all angles, CT scans are much more comprehensive and provide much more information than conventional X-rays. They are ideal to use in emergency situations, and take approximately 5 minutes to complete.\n",
    "\n",
    "The problems that can be detected by using a CT scan include bone injuries and related issues, head trauma, and problems related to the pelvis, lungs, and abdomen. CTs are also incredibly useful for diagnosing and staging cancer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb2736f",
   "metadata": {
    "id": "5cb2736f"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a96f99",
   "metadata": {
    "id": "60a96f99"
   },
   "source": [
    "## Magnetic Resonance Imaging (MRI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d73253",
   "metadata": {
    "id": "e0d73253"
   },
   "source": [
    "<img src=\"materials/images/mri.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cf23da",
   "metadata": {
    "id": "04cf23da"
   },
   "source": [
    "Magnetic Resonance Imaging (MRI) uses technology that relies on radio waves and magnets rather than the X-rays used by radiography. A constant magnetic field and radio frequencies bounce off of the fat and water molecules in your body. Radio waves are transmitted to a receiver in the machine which is translated into an image that can be used to diagnose issues.\n",
    "\n",
    "Like the CT scan, for an MRI, the patient lies down on a table that is moved through a doughnut-shaped structure as images are acquired. Same to the CT scan, MRI technology enables two-dimensional, trans-axial slices, and can generate three-dimensional images of the scanned region.\n",
    "\n",
    "However, although MRI technology produces similar types of images as a CT scan, the appearance of the tissue differs between the two technologies. MRI scans tend to have greater detail and differentiation of soft tissue structures than can be obtained with CT scanning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e2d811",
   "metadata": {
    "id": "49e2d811"
   },
   "source": [
    "<img src=\"materials/images/ct-vs-mri.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8313b0e0",
   "metadata": {
    "id": "8313b0e0"
   },
   "source": [
    "MRI scans are best suited for looking at soft tissues (e.g. ligaments and tendons), spinal cord, and brain tissue to reveal tumors and multiple sclerosis. MRI can provide excellent detail of these structures that allows radiologists to diagnose a wide variety of disorders.\n",
    "\n",
    "An advantage of an MRI is that there is no radiation used. However, the scan takes 30 to 60 minutes, much longer than a CT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0982f31a",
   "metadata": {
    "id": "0982f31a"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> Although <b>MRI</b> and <b>CT</b> scans are similar procedures, they are not the same tests. Both provide diagnostic images, but each exam is ordered based on what you can see more clearly on the images. In some cases, both may be needed. CT scans are usually the first choice for imaging. MRIs are useful for certain diseases that a CT scan cannot detect.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f5f482",
   "metadata": {
    "id": "11f5f482"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128e7f98",
   "metadata": {
    "id": "128e7f98"
   },
   "source": [
    "# Ultrasound"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb55ac0",
   "metadata": {
    "id": "4fb55ac0"
   },
   "source": [
    "<img src=\"materials/images/ultrasound.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179252ae",
   "metadata": {
    "id": "179252ae"
   },
   "source": [
    "An ultrasound is a very different form of medical imaging than the previous methods. It uses sound waves (sonar) to create an image (also known as a sonogram) of organs, tissues, and other structures inside the body.\n",
    "\n",
    "During an ultrasound, the patient will lie on a table, exposing the area that’s being viewed. A health care provider will spread a special gel on the skin over that area. The provider will then move a wand-like device, called a transducer, over the area. The device sends sound waves into your body. The waves are recorded and turned into images on a monitor.  \n",
    "\n",
    "There are two main categories of ultrasounds: pregnancy ultrasound and diagnostic ultrasound.\n",
    "\n",
    "- **Pregnancy ultrasound** is used to confirm a pregnancy, or to get information about a baby’s growth, development, and overall health.\n",
    "\n",
    "- **Diagnostic ultrasound** is used to view and provide information about other internal parts of the body. These include the heart, blood vessels, liver, bladder, kidneys, and female reproductive organs.\n",
    "\n",
    "As with MRI technology, ultrasounds don’t involve any radiation exposure.\n",
    "\n",
    "Further, ultrasounds have the advantage of producing real-time video, and showing parts of the body in motion, such as a heart beating or blood flowing through blood vessels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67525e43",
   "metadata": {
    "id": "67525e43"
   },
   "source": [
    "<img src=\"materials/images/animated_sonogram.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c49e07e",
   "metadata": {
    "id": "8c49e07e"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae3ebdf",
   "metadata": {
    "id": "7ae3ebdf"
   },
   "source": [
    "# Using Machine Learning for Medical Image Classification\n",
    "\n",
    "The use of medical imaging to assess anomaly and provide diagnosis is a standard part of medical care. The addition of artificial intelligence and machine learning to aid in this process has been transformative.\n",
    "\n",
    "Machine learning models, trained effectively on sample images, can make virtually the same diagnoses as a radiologist, and perhaps even better. Moreover, machine learning models can \"read\" images in much less time, and can be used to provide more consistent diagnoses. Further, this efficiency can lower the cost of using medical imaging for doctors and patients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da858bfb",
   "metadata": {
    "id": "da858bfb"
   },
   "source": [
    "## Teaching the machine how to interpret medical images\n",
    "\n",
    "Machines learn by example, so our task is to present the machine with hundreds or even thousands of example images, labeled according to their respective diagnoses (i.e., classification). A machine learning model will then learn the most important features in an image associated with its label (i.e., diagnosis). Deep learning is the most effective method to accomplish this. We will dive into it next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0dc884",
   "metadata": {
    "id": "ab0dc884"
   },
   "source": [
    "<img src=\"materials/images/sample-xray.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312857ba",
   "metadata": {
    "id": "312857ba"
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "Neural networks are a machine learning method, loosely based on the human brain, that uses interconnected nodes or neurons in a layered structure. A Convolutional Neural Network (CNN) is a specialized type of neural network that is highly effective for image classification. As an image is passed through each successive (\"Conv\") layer of the CNN (see image below), more refined features are extracted that are related to the appropriate classification. The features may be a shape, color, location, or some combination of each. It is these layers, the depth of this structure, that gives us the term **\"deep learning\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d41c879",
   "metadata": {
    "id": "1d41c879"
   },
   "source": [
    "<img src=\"materials/images/cnn.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a600810",
   "metadata": {
    "id": "2a600810"
   },
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> In the above image, the CNN learns to extract the most important features from the chest X-ray associated with the given label (e.g., normal, pneumonia). Each successive \"Conv\" layer extracts more refined features from the image. When the final layer is reached, the most important features have been extracted, and the model uses that information to make its predicted classification.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c36d578",
   "metadata": {
    "id": "4c36d578"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f046c8",
   "metadata": {
    "id": "61f046c8"
   },
   "source": [
    "# Train a deep learning model\n",
    "\n",
    "We will use TensorFlow, a deep learning library, to train a model to recognize images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a6a5be",
   "metadata": {
    "id": "06a6a5be"
   },
   "source": [
    "## 1. Medical imaging dataset\n",
    "\n",
    "**Machines learn by example**, so we first need to provide the machine with examples of our task that it can learn from.\n",
    "\n",
    "We're going to use a dataset consisting of **normal** and **pneumonia** [chest X-ray images](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia). Typically, for machine learning, we desire a training set of images for the model to learn from, and a test set of \"unseen\" images to be used to evaluate the model's performance. We will be using 1,000 **training images** (500 from each class, \"normal\" versus \"pneumonia\"), and 14 **test images** to evaluate the performance of our model once it has been trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71b9115",
   "metadata": {
    "id": "d71b9115"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Alert:</b> Typically, you would use approximately a 70/30 split for your data (70% of the data would be used for training the model, and 30% of the data would be held back as a test set to evaluate the model's performance after training). Then, after training, you would usually just get an overall accuracy score for how the model performed, collectively, on all of the test images. In our case, for demonstration purposes, we used a smaller test set (14 images) so that at the end of the module you will be able to see the predictions that the trained model makes on each of the 14 test images.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ae6999",
   "metadata": {
    "id": "17ae6999"
   },
   "source": [
    "### 1.1 Preview of chest X-rays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b25839",
   "metadata": {
    "id": "e9b25839"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Location of images\n",
    "path = 'data/train/Normal/'\n",
    "image_files = os.listdir(path)\n",
    "\n",
    "print('Sample Images')\n",
    "\n",
    "# Set the size of the images\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "# Select and display random images\n",
    "for i in range(4):\n",
    "    plt.subplot(2, 2, i + 1)\n",
    "    img = plt.imread(os.path.join(path, image_files[random.randrange(0, len(image_files))]))\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "# Adjust subplot parameters so that images are more evenly padded\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac42510",
   "metadata": {
    "id": "eac42510"
   },
   "source": [
    "### 1.2 Direct TensorFlow to our images\n",
    "\n",
    "We'll use TensorFlow's ImageDataGenerator to locate, and prepare our images for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27c5df8",
   "metadata": {
    "id": "b27c5df8"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Normalize images: normalization typically entails scaling values to between 0 and 1.\n",
    "                  # Images are made of pixels with varying intensities from 0 to 255.\n",
    "                  # So normalizing the images (dividing each pixel by 255) scales the pixel values to between 0 and 1.\n",
    "                  # This range of values is more effective for a neural network to learn from.\n",
    "train_data_generator = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "# Indicate the location of our images\n",
    "train_folder = \"data/train\"\n",
    "\n",
    "# Provide images to the machine in batches using the train_data_generator\n",
    "train_generator = train_data_generator.flow_from_directory(\n",
    "        train_folder,  # This indicates where images are located\n",
    "                        # It contains folders, whose names will be the class labels, that contain the images\n",
    "        target_size=(150, 150), # All images will be resized to 150x150\n",
    "        batch_size=100,        # The generator will provide the model with 100 images at a time as it's \"learning\"\n",
    "        class_mode=\"binary\")   # We use \"binary\" because there are two classes (i.e. normal, pneumonia)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eb29dd",
   "metadata": {
    "id": "e9eb29dd"
   },
   "source": [
    "Notice the feedback above: we're told that our 1,000 training images belonging to 2 classes (i.e., normal, pneumonia) were found. **Everything is as expected!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca309ce",
   "metadata": {
    "id": "6ca309ce"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5955fdcd",
   "metadata": {
    "id": "5955fdcd"
   },
   "source": [
    "## 2. Build an AI model to learn from our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a73b99",
   "metadata": {
    "id": "99a73b99"
   },
   "source": [
    "### 2.1 Build a Convolutional Neural Network (CNN)\n",
    "\n",
    "Envision the following layers as matching the preceding CNN image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dc5c32",
   "metadata": {
    "id": "22dc5c32"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers\n",
    "\n",
    "model = models.Sequential([\n",
    "    # The input shape is the shape of all images (150x150); there are 3 color channels (RGB)\n",
    "    # Layer 1\n",
    "    layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    layers.MaxPooling2D(),\n",
    "\n",
    "    # Layer 2\n",
    "    layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "\n",
    "    # Layer 3\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "\n",
    "    # Layer 4\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "\n",
    "    # Layer 5\n",
    "    layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "\n",
    "    # Flatten (combine) the learned features for prediction\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'), # 'relu' helps the model focus on what's most important and speeds up training\n",
    "    layers.Dense(1, activation='sigmoid') # 'sigmoid' ensures that the prediction will be between 0 and 1,\n",
    "                                            # interpreted as the probability of being in the positive class.\n",
    "])\n",
    "\n",
    "\n",
    "# 'binary_crossentropy' (this is a binary task) tells the model how to measure its loss (error)\n",
    "# 'adam' indicates how the model will adjust it weights, while learning, to decrease its loss (error)\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5788e9",
   "metadata": {
    "id": "6f5788e9"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> The above model uses Conv2D (Convolutional) layers to learn to extract the most important features related to a given label. The model uses Conv2D layers because we are training on radiographs (chest X-rays) which are two dimensional.  If you wanted to train on CT or MRI images whose slices were combined into a three-dimensional image, you could choose to use Conv3D layers.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537d29ae",
   "metadata": {
    "id": "537d29ae"
   },
   "source": [
    "### 2.2. Train the CNN to use our sample images\n",
    "\n",
    "Now that the algorithm is built, let's train it to differentiate between normal and pneumonia chest X-rays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f736072",
   "metadata": {
    "id": "6f736072"
   },
   "source": [
    "`model.fit()` instructs the model to begin training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7684b9",
   "metadata": {
    "id": "4f7684b9"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "      train_generator,\n",
    "      steps_per_epoch=10, # The total number of batches (of 100) required to \"see\" all 1000 of the training images.\n",
    "      epochs=10)          # The number of times the model will get to go through all of the images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2816f93",
   "metadata": {
    "id": "d2816f93"
   },
   "source": [
    "### 2.3 Understand the output above\n",
    "\n",
    "As the model begins training, it receives 100 random images at a time (we previously set the batch size to 100). It then makes predictions on those 100 images. It compares its predictions to the actual labels, notes its error, and then adjusts its internal parameters (weights) to reduce its error. It is attempting to learn the best features that will enable it to correctly classify the images.\n",
    "\n",
    "It then will receive the next batch of 100 images. This continues until the 1,000 images are exhausted.\n",
    "\n",
    "We have set the number of epochs to 10. It means that the model can go through all 1,000 images ten times, learning as it goes. Each line is an epoch. The next line is the model going through all 1,000 of the images again (100 at a time).\n",
    "\n",
    "Note the **accuracy** on the far right. This indicates the proportion of the model's predictions that were classified correctly. This should improve with each epoch (loop through all of the images). The model's **loss** indicates its error when making predictions. As loss drops, accuracy increases. Accuracy is probably a fair metric to use here as the dataset is balanced (500 in each class). If it were imbalanced, precision or recall metrics would prove more useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2455hQPD9aW-",
   "metadata": {
    "id": "2455hQPD9aW-"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> The model above is being evaluated only on the training set of images. Typically, the performance on a validation set of images (additional images the model had not previously been trained on) would also be monitored during this process. This will be demonstrated in the dedicated deep learning module.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162f6293",
   "metadata": {
    "id": "162f6293"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3c44f3",
   "metadata": {
    "id": "ca3c44f3"
   },
   "source": [
    "## 3. Evaluate the model's performance on test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7f72c9",
   "metadata": {
    "id": "5f7f72c9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#  The location of our test (unseen) images, used to evaluate the model's predictions\n",
    "test_images = os.listdir(\"data/test\")\n",
    "\n",
    "# Make predictions on the test images\n",
    "for file_name in test_images:\n",
    "    path = \"data/test/\" + file_name\n",
    "    test_img = image.load_img(path, target_size = (150,150))  # Resize images to the same size the model trained on\n",
    "    img = image.img_to_array(test_img)\n",
    "    img /= 255.0    # Normalize the images\n",
    "    img = np.expand_dims(img, axis = 0)\n",
    "\n",
    "  # Predict an image's class\n",
    "    prediction = model.predict(img)\n",
    "\n",
    "  # Output the prediction score\n",
    "    print(f\"Prediction: {prediction[0]}\") # The model's probability score that the image is \"Pneumonia\"\n",
    "                                        # Probability < 0.5 predicts \"Normal\"; .05 or greater predicts \"Pneumonia\"\n",
    "  # Output the file name\n",
    "    if prediction[0] < 0.5:\n",
    "        print(file_name + \" is normal\")\n",
    "\n",
    "    else:\n",
    "        print(file_name + \" is pneumonia\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53930cc5",
   "metadata": {
    "id": "53930cc5"
   },
   "source": [
    "### Evaluate the above predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90652985",
   "metadata": {
    "id": "90652985"
   },
   "source": [
    "Above are predictions on 14 test images that the model has never \"seen\".\n",
    "\n",
    "An image with \"normal\" in its name should be predicted to be in the class \"normal\", and an image with \"pneumonia\" in its name should be predicted to be in the class \"pneumonia\".\n",
    "\n",
    "The \"Prediction\" value indicates the model's confidence (probability) that the image is of pneumonia:\n",
    "- Closer to 0, the model predicts normal.\n",
    "- Closer to 1, the model predicts pneumonia.\n",
    "\n",
    "Prediction is the goal of machine (deep) learning. The quality of the model's predictions are what the various evaluation metrics (e.g., accuracy, precision, recall) are evaluating.  A model's performance can be improved by providing it with more images, or by tuning the model's parameters (e.g., modifying the number of layers).\n",
    "\n",
    "Additionally, we would typically use a validation set of images to provide us with additional feedback on the model's performance, while it's being trained. We will elaborate on that in another module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9bcb56",
   "metadata": {
    "id": "1d9bcb56"
   },
   "source": [
    "#### You should now have a basic understanding of using machine learning to predict the classification of medical images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef467ebd",
   "metadata": {
    "id": "ef467ebd"
   },
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cfa04d",
   "metadata": {
    "id": "a6cfa04d"
   },
   "source": [
    "# Contributions & acknowledgement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215e49ef",
   "metadata": {
    "id": "215e49ef"
   },
   "source": [
    "Thank the following team for working on this module:\n",
    "\n",
    "- **Module Content**: Antony Ross\n",
    "- **Engineering**: Amit Dixit\n",
    "- **UX/UI Design & Illustration**: Kexin Cha, Antony Ross\n",
    "- **Video Production**: Francesca Goncalves\n",
    "- **Project Management**: Amir Bahmani, Kexin Cha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fxF28xJndo5t",
   "metadata": {
    "id": "fxF28xJndo5t"
   },
   "source": [
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SWGcIuMDdqes",
   "metadata": {
    "id": "SWGcIuMDdqes"
   },
   "source": [
    "Data contribution:\n",
    "\n",
    "- Data: https://data.mendeley.com/datasets/rscbjbr9sj/2\n",
    "- License: CC BY 4.0\n",
    "- Citation: http://www.cell.com/cell/fulltext/S0092-8674(18)30154-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86062d40",
   "metadata": {
    "id": "86062d40"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8415678",
   "metadata": {
    "id": "e8415678"
   },
   "source": [
    "Copyright (c) 2023 Stanford Data Ocean (SDO)\n",
    "\n",
    "All rights reserved."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
